## File: dnsmasq_log_reader_v002
# 
# HEAVILY COMMENTED BECAUSE I mean to explain stuff as I go.
# But not meant to be kept so verbose.
# Uses dnsmasq log entries registered in a syslog file(s).
# Focus of this version of the script is still on dnsmasq queries only.
# This script:
# - reads one or more syslog files from a directory
# - filters and parses dnsmasq queries from the logs
# - extracts domains to their second level (can be improved upon...)
# - uses Rwhois package to gather creation dates for queried domains.
# Version 0.2:
# - More functional programming, some Closures using environment-restricted variables.
# - Added Whois context data gathering.

# Load all libraries at the beginning:
library(dplyr)
library(stringr)
library(lubridate)
# Lubridate cheat sheet: https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf
library(ggplot2)
library(Rwhois) # To supplement Domain Data
#library(data.table) # Not used after all for the exercise (so far).
# library('visNetwork') # For later versions
# library('shiny') # For later versions

##
# EDIT HERE
##

# For the exercise, I gathered some /var/log/syslog files locally (we'll see later on how to evolve that)
## Your directory with syslog files:
dns_logs_dir <- "/mnt/R/data/"
# In the syslog files, I expect to see entries looking like so:
#
# Dec  5 00:05:06 linuxservername dnsmasq[898]: query[A] www.apple.com from 10.0.0.2
# Dec  5 00:05:06 linuxservername dnsmasq[898]: query[A] api.apple-cloudkit.com from 10.0.0.2
#

# Used to avoid querying all domains again:
# A file in CSV format with domain,date_create columns:
domain_created_date_context_data_file <- "/mnt/R/domains_created.csv"
# Sample for format:
#
# "domain","created_date"
# "apple.com","1987-02-19T05:00:00Z"
# "akadns.net","1999-05-12T21:15:16Z"
#

##
# "Functions, Functions Everywhere" (Functional Programming concepts put to use)
##

# Why a function here? To make sure I adapt it if it doesn't work on another system...
add_year_to_log_lines_by_file_date <- function(filename) {
  year(file.info(filename)$mtime-86400)
  # Logs in a file after rotate are from the day before
}

# Reads all files in a directory. Could be improved, as this is missing
# input validations...
# Called from a lapply with list.files(dns_logs_dir), so at least in this instance
# files should indeed exist... But really needs improvement.
gather_log_lines <- function(t_filename) {
  tempfile <- paste0(dns_logs_dir, t_filename)
  # First of, I have an issue with the logs: In the format I have them, the year is not informed.
  # As we are already in December, this will soon be a problem...
  # So I'll add the year of the file to each log entry like so: (requires lubridate package)
  paste(add_year_to_log_lines_by_file_date(tempfile),
        readLines(tempfile, warn = FALSE))
}

# To use on raw log of queries generated by a dnsmasq:
parse_dns_logs_queries <- function(queries_logs) {
  # We prepare a data.frame to work with:
  dns_logs_queries <- data.frame(log = queries_logs, req.ip = "", domain = "", 
                                 stringsAsFactors = FALSE)
  # The requesting IP address appears at the very end of the string.
  # One way to go about it is a to look for "everything not a space until the end":
  dns_logs_queries$req.ip <- with(dns_logs_queries, str_extract(log, "[^ ]+$"))
  
  # Check https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf for good reference
  # Another way to go about extracting the domain name from the string:
  # So what we do is first look for everything after the last "]" character:
  dns_logs_queries$domain <- with(dns_logs_queries, str_extract(log, "[^\\]]+$"))
  # and then everything not a space character after the first " ":
  dns_logs_queries$domain <- str_extract(dns_logs_queries$domain, "[^ ]+")
  # Now domain contains the FQDN
  
  # Now let's add the date & time
  dns_logs_queries$datetime <- with(dns_logs_queries, str_sub(log, 1, 20))
  # Date might have up to two spaces (because of 1 or 2 digits day number)
  dns_logs_queries$datetime <- sub("  ", " ", dns_logs_queries$datetime)
  dns_logs_queries$datetime <- ymd_hms(dns_logs_queries$datetime)
  
  # Now we're ready to get rid of the log itself
  dns_logs_queries <- dns_logs_queries[,-1]
  
  # Let's add some useful variables for visualization:
  dns_logs_queries$date <- date(dns_logs_queries$datetime)
  dns_logs_queries$hour <- hour(dns_logs_queries$datetime)
  
  # Next, let's look at the domains:
  # Let's first get all the levels of the FQDN, separated by "."
  dns_logs_queries$domain_levels <- strsplit(dns_logs_queries$domain, "\\.")
  
  dns_logs_queries # return
}

# For more on this next bit, "Closures", see:
# http://adv-r.had.co.nz/Functional-programming.html
domain_sublevel_n <- function(n) {
  function(x) {
    x_l <- length(x)
    ifelse(x_l >= n,
           paste(x[(x_l-n+1):x_l], collapse = "."),
           NA
    )
  }
}

# Another Closure, to keep contextual data loaded from Cache available when called:
closure_domain_created_date <- function(cachefile = "") {
  
  # Why a cache file?
  # One should expect users consult repeatdly the same domains, and so
  # there would be no need to consult new domain creation dates:
  cache_data <- NULL
  if(cachefile != "") {
    tryCatch(
      {
        cache_data <- read.csv(cachefile)
      },
      error=function(cond) {
        message(paste("Error: Cache not read"))
        message(cond)
        # Do not return
      },
      warning=function(cond) {
        message(paste("Warning: Cache not read"))
        message(cond)
        # Do not return
      },
      finally={
        message(paste("Cache_data NULL? ", is.null(cache_data)))
      })
    if(!is.null(cache_data)) {
      cache_data <- cache_data[!is.na(cache_data$domain),]
    }
  }
  
  # cache_data is available within the Closure context:
  function(domain_name) {
    
    # Do not query Whois if the domain was already consulted in the past:
    if(!is.null(cache_data)) {
      if(domain_name %in% cache_data$domain) {
        cache_entry_date <- cache_data[cache_data$domain == domain_name, "created_date"]
        message(paste(domain_name, 
                      "found in cache data. Not querying Whois. Date found: ", 
                      cache_entry_date))
        flush.console() # Cool debugging trick from within functions/loops.
        return(cache_entry_date)
      }
    }
    
    value_date <- ""
    temp <- NULL
    
    tryCatch(
      {
        temp <- whois_query(domain_name) # Modify: Careful with CNAMEs here...
      },
      error=function(cond) {
        message(paste("Error: Whois Error"))
        message(cond)
        str(temp)
        message(temp)
        # Do not return
      },
      warning=function(cond) {
        message(paste("Warning: Whois Error"))
        message(cond)
        # Do not return
      },
      finally={
      })
    
    if(!is.null(temp) && nrow(temp[temp$key == "Creation Date",]) >= 1) {
      value_date <- as.character(temp[temp$key == "Creation Date","val"])
      #domain_created_date[domain_created_date$domain == i,]$created_date <- value_date
    }
    
    print(paste("In: ", domain_name, value_date))
    flush.console()
    value_date
  }
}

##
# END Functions
##

all_logs <- lapply(list.files(dns_logs_dir), gather_log_lines)
# We have a list of entries per file read:
all_logs <- unlist(all_logs)

# Two step grep, so that the next few greps are only run on the relevant subset:
dns_logs <- all_logs[grep("dnsmasq.*", all_logs)]
rm(all_logs) # Minimal cleaning up

# For later use, we gather reply data:
dns_logs_cnames <- dns_logs[grep("is <CNAME>$", dns_logs)]
# Not necessarily good, as not valid for IPv6:
dns_logs_reps <- dns_logs[grep("reply.* is .*[0-9]", dns_logs)]
# Cached replies count too, for us:
dns_logs_cached <- dns_logs[grep("cached.* is .*", dns_logs)]



# Now from all the logs, we keep only those that include a query:
dns_logs_queries <- dns_logs[grep("query\\[", dns_logs)]

dns_logs_queries <- parse_dns_logs_queries(dns_logs_queries)


##
# Some visualizations:
##
# Let's check volume of queries per day first:
dns_logs_queries_days <- dns_logs_queries %>% group_by(date) %>% tally()
days_dns_plot <- ggplot(dns_logs_queries_days, aes(date, n, group = 1)) + geom_line()
days_dns_plot
# As I don't have much spare time, I don't test the server as consistently as I'd like...

dns_logs_queries_hours <- dns_logs_queries %>% group_by(hour) %>% tally()
hours_dns_plot <- ggplot(dns_logs_queries_hours, aes(hour, n)) + geom_line()
hours_dns_plot
# It seems like I do most of my testing with the server early in the mornings...

# Using Closure for getting sub-domain level x:
first_level <- domain_sublevel_n(1)
sapply(dns_logs_queries$domain_levels, first_level)

second_level <- domain_sublevel_n(2)
# Let's keep all unique dns domains level 2:
l2_domain <- unique(sapply(dns_logs_queries$domain_levels, 
                           second_level)
)
# And so we could go on.

##
# Supplementing data:
##

# With domain names, we can get registered dates using Whois.
# needed: library(Rwhois), loaded at the beginning.
domain_created_date <- data.frame(domain = l2_domain,
                                  created_date = rep("",
                                                     length(l2_domain)))
# Some cleaning up just in case:
domain_created_date <- domain_created_date[!is.na(domain_created_date$domain),]

# Third version: Using Closure Environment to load whois cache data
# Whois is rather slow (for real-time use over many domain names, that is), 
# impacting the loading of contextual data.
#
# What we do here is create a Closure that will load cache data.
#
# Then we create a function that uses that context data, in the function's environment.
# If a domain is found in the "cache file", it is not queried again.
# This should save time of execution, allowing to reload only new Level2s of FQDNs.
get_domain_created_date <- closure_domain_created_date(domain_created_date_context_data_file)

# # A few cases to test our Closure for our validations through tryCatch:
# test_closure_domains_created <- c("google.es", "apple.com", "amazon.com")
# sapply(test_closure_domains_created, get_domain_created_date)

# OK now load date of creation for each domain observed in the DNS Queries (Level 2):
domain_created_date$created_date <- sapply(domain_created_date$domain,
                                           get_domain_created_date)

# Overwrite Cache data, so that we can avoid querying again ALL of them in the future:
write.csv(domain_created_date,
          file = domain_created_date_context_data_file,
          row.names = FALSE)
# Note to self: Maybe a better version would include the date of caching...
# So that I can update some of the "older" entries... Implementing an actual Cache.

# Now what's good is, we now have a MongoDB Container next to us... Let's try to use that:
# sudo docker ps -a
# CONTAINER ID   IMAGE            COMMAND                  CREATED             STATUS             PORTS                               NAMES
# f32e997931d7   rocker_base001   "/init"                  26 minutes ago      Up 26 minutes      0.0.0.0:8787->8787/tcp              rocker1
# 56bec5382050   mongo            "docker-entrypoint.sâ€¦"   About an hour ago   Up About an hour   27017/tcp, 0.0.0.0:1235->8081/tcp   mongo1
library(mongolite)
# Default, NOT SECURE:
m <- mongo(collection = "domains_date",  db = "DNS", url = "mongodb://mongo1")
print(m)
m$insert(domain_created_date)
# Everything went OK?
m$count() == nrow(domain_created_date)
